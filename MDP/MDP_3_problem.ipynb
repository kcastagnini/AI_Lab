{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning-Lab Lesson 3: Markov Decision Process\n",
    "\n",
    "In the third session we will work on the Markov decision process (MDP)\n",
    "\n",
    "## Lava environments\n",
    "The environments used are LavaFloor (visible in the figure) and its variations.\n",
    "\n",
    "![Lava](images/lava.png)\n",
    "\n",
    "The agent starts in cell $(0, 0)$ and has to reach the treasure in $(2, 3)$. In addition to the walls of the previous environments, the floor is covered with lava, there is a black pit of death.\n",
    "\n",
    "Moreover, the agent can't comfortably perform its actions that instead have a stochastic outcome (visible in the figure):\n",
    "\n",
    "![Dynact](images/dynact.png)\n",
    "\n",
    "The action dynamics is the following:\n",
    "- $P(0.8)$ of moving **in the desired direction**\n",
    "- $P(0.1)$ of moving in a direction 90Â° with respect to the desired direction\n",
    "\n",
    "Finally, since the floor is covered in lava, the agent receives a negative reward for each of its steps!\n",
    "\n",
    "- -0.04 for each lava cell (L)\n",
    "- -5 for the black pit (P). End of episode\n",
    "- +1 for the treasure (G). End of episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "module_path = os.path.abspath(os.path.join('../tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Properties \n",
    "\n",
    "In addition to the varables of the environments you have been using in the previous sessions, there are also a few more:\n",
    "\n",
    "- $T$: matrix of the transition function $T(s, a, s') \\rightarrow [0, 1]$\n",
    "- $RS$: matrix of the reward function $R(s) \\rightarrow \\mathbb{R}$\n",
    "\n",
    "The available actions are still Left, Right, Up, Down.\n",
    "\n",
    "#### Code Hints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions:  4\n",
      "Actions:  {0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n",
      "Reward of starting state: -0.04\n",
      "Reward of goal state: 1.0\n",
      "Probability from (0, 0) to (0, 1) with action right: 0.8\n",
      "Probability from (0, 0) to (2, 3) with action right: 0.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LavaFloor-v0\")\n",
    "\n",
    "current_state = env.pos_to_state(0, 0)\n",
    "next_state = env.pos_to_state(0, 1)\n",
    "goal_state = env.pos_to_state(2, 3)\n",
    "\n",
    "print(\"Number of actions: \", env.action_space.n)\n",
    "print(\"Actions: \", env.actions)\n",
    "print(\"Reward of starting state:\", env.RS[current_state])\n",
    "print(\"Reward of goal state:\", env.RS[goal_state])\n",
    "print(\"Probability from (0, 0) to (0, 1) with action right:\", env.T[current_state, 1, next_state])\n",
    "print(\"Probability from (0, 0) to (2, 3) with action right:\", env.T[current_state, 1, goal_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of time agent reaches the state to the right:  76.0\n",
      "Transition model for  LavaFloor-v0  : \n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  L  :  0.0\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  R  :  0.8\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  U  :  0.1\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  D  :  0.1\n",
      "Reward for non terminal states:  -0.04\n",
      "Reward for state : (1, 3)  (state type:  P ) :  -5.0\n",
      "P for Pozzo, G for Goal\n",
      "Reward for state : (2, 3)  (state type:  G ) :  1.0\n",
      "P for Pozzo, G for Goal\n"
     ]
    }
   ],
   "source": [
    "env_name = \"LavaFloor-v0\"\n",
    "env = gym.make(env_name)\n",
    "\n",
    "c=0\n",
    "state_right = env.pos_to_state(0, 1) #state to the tight of start state\n",
    "for i in range(1,101):\n",
    "    current_state = env.pos_to_state(0, 0)\n",
    "    state = env.sample(current_state, 1) #trying to go right\n",
    "    if (state==state_right): \n",
    "        c+=1 #counting how many times the agent reaches the state to the right\n",
    "        \n",
    "#computing percentage of time agent reached the state to the right going right, should be around 80%           \n",
    "print(\"percentage of time agent reaches the state to the right: \", c/i*100) \n",
    "\n",
    "print(\"Transition model for \", env_name, \" : \") #assume transition functions is the same for all states\n",
    "state=0\n",
    "next_state=1\n",
    "for i in range(0,env.action_space.n):\n",
    "    print(\"probability of reaching \", env.state_to_pos(next_state), \"from \", env.state_to_pos(state), \" executing action \", env.actions[i], \" : \", env.T[state, i, next_state])\n",
    "print(\"Reward for non terminal states: \",env.RS[env.pos_to_state(0,0)]) #assume all states have same reward\n",
    "for state in range(0,env.observation_space.n):\n",
    "    if env.grid[state] == \"P\" or env.grid[state] == \"G\":\n",
    "        print(\"Reward for state :\", env.state_to_pos(state) ,\" (state type: \", env.grid[state],\") : \",env.RS[state])\n",
    "        print(\"P for Pozzo, G for Goal\")\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Value Iteration Algorithm\n",
    "\n",
    "Your first assignment is to implement the Value Iteration algorithm on LavaFloor. The solution returned by your algorithm must be a 1-d array of action identifiers where the $i$-th action refers to the $i$-th state.  You can perform all the test on a different versions of the environment, but with the same structure: *HugeLavaFloor*, *NiceLavaFloor* and *VeryBadLavaFloor*.\n",
    "\n",
    "<img src=\"images/value-iteration.png\" width=\"600\">\n",
    "\n",
    "The *value_iteration* function has to be implemented. Notice that the value iteration approach return a matrix with the value for eacht state, the function *values_to_policy* automatically convert this matrix in the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a state S, returns the max of a list where each element is a sum of the utilities of the states reached with each action from S, \n",
    "#multiplied by the probability of the action \n",
    "def maxOfActions(currentState, environment, U):\n",
    "\n",
    "    #print(\"env.actions:\")\n",
    "    #for action in environment.actions:\n",
    "    #    print (\"action: \", action)\n",
    "\n",
    "    #print(\"env.action_space: \", environment.action_space)\n",
    "\n",
    "    if environment.grid[currentState] == \"P\" or environment.grid[currentState] == \"G\":\n",
    "        return 0\n",
    "    \n",
    "    res = list(range(environment.action_space.n)) #list of length of number of actions possible\n",
    "\n",
    "    #debug:\n",
    "    #environment.T[environment.pos_to_state(2, 0), ]\n",
    "\n",
    "    for action in range(environment.action_space.n): #this is the outer loop, correct\n",
    "\n",
    "        #now I need a inner loop for each other possible state\n",
    "\n",
    "        val = 0\n",
    "        \n",
    "        for nextState in environment.staterange: #This iterate through all states (including the one not reachable because distant more than 1 cell)\n",
    "      \n",
    "            # s' = nextState, s = currentState, a = action\n",
    "            # P(s' | s, a) * U[s']        \n",
    "            prob = environment.T[currentState, action, nextState] # P(s' | s, a)\n",
    "            val += prob * U[nextState] # U[s']\n",
    "        \n",
    "            #print(f\"prob from state {environment.state_to_pos(currentState)} with action {environment.actions[action]} to state {environment.state_to_pos(nextState)} is {prob}\")\n",
    "            \n",
    "        res[action] = val #result is indexed by action index\n",
    "\n",
    "    #max(P(s' | s, a) * U[s'])\n",
    "    return max(res)\n",
    "\n",
    "def value_iteration(environment, maxiters=300, discount=0.5, max_error=1e-3):\n",
    "    \"\"\"\n",
    "    Performs the value iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        maxiters: timeout for the iterations\n",
    "        discount: gamma value, the discount factor for the Bellman equation\n",
    "        max_error: the maximum error allowd in the utility of any state\n",
    "        \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"using discount\", discount)\n",
    "    \n",
    "    U_1 = [0 for _ in range(environment.observation_space.n)] # vector of utilities for states S\n",
    "    U = U_1.copy()\n",
    "    #\n",
    "    # Code Here!\n",
    "    #\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    #this is the repeat-until in the pseudo code\n",
    "    while True:\n",
    "\n",
    "        U = U_1.copy()\n",
    "        delta = 0 # maximum change in the utility o any state in an iteration\n",
    "\n",
    "        #for s in environment\n",
    "\n",
    "        for s in environment.staterange:\n",
    "                          \n",
    "            U_1[s] = environment.RS[s] + discount * maxOfActions(s, environment, U)\n",
    "\n",
    "            deltaU = abs(U_1[s] - U[s])\n",
    "            \n",
    "            if deltaU > delta:\n",
    "                delta = deltaU\n",
    "\n",
    "        #print(\"obs_space\", env.observation_space)\n",
    "        #print(\"obs_space.n\", env.observation_space.n)      \n",
    "        #print(\"states range: \", env.staterange)\n",
    "\n",
    "        #for s in environment.staterange:\n",
    "        #    print(\"state: \", environment.state_to_pos(s))\n",
    "                #break\n",
    "\n",
    "        if delta < max_error * (1 - discount) / discount:\n",
    "            print(\"Exiting with delta: \", delta)\n",
    "            break\n",
    "        \n",
    "        #CODE BELOW IS TO PREVENT INFINITE LOOPS\n",
    "        i += 1\n",
    "        \n",
    "        if i >= maxiters:\n",
    "            print(\"BLAM! maxiters REACHED\")\n",
    "            break\n",
    "\n",
    "    for u in range(0, len(U_1)):\n",
    "        print(f\"Utility at pos {environment.state_to_pos(u)} is {U_1[u]}\")\n",
    "    \n",
    "    return values_to_policy(np.asarray(U), env) # automatically convert the value matrix U to a policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code executes Value Iteration and prints the resulting policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENV RENDER:\n",
      "[['S' 'L' 'L' 'L']\n",
      " ['L' 'W' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n",
      "Transition model for  LavaFloor-v0  : \n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  L  :  0.0\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  R  :  0.8\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  U  :  0.1\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  D  :  0.1\n",
      "Reward for non terminal states:  -0.04\n",
      "Reward for state : (1, 3)  (state type:  P ) :  -5.0\n",
      "Reward for state : (2, 3)  (state type:  G ) :  1.0\n",
      "using discount 0.5\n",
      "Exiting with delta:  0.0009554359999999901\n",
      "Utility at pos (0, 0) is -0.063568364\n",
      "Utility at pos (0, 1) is -0.072960524\n",
      "Utility at pos (0, 2) is -0.06394412\n",
      "Utility at pos (0, 3) is -0.078441776\n",
      "Utility at pos (1, 0) is -0.041249867999999995\n",
      "Utility at pos (1, 1) is 0.0\n",
      "Utility at pos (1, 2) is -0.0407390885\n",
      "Utility at pos (1, 3) is -5.0\n",
      "Utility at pos (2, 0) is 0.007469268000000008\n",
      "Utility at pos (2, 1) is 0.12300467200000001\n",
      "Utility at pos (2, 2) is 0.37679193750000006\n",
      "Utility at pos (2, 3) is 1.0\n",
      "\n",
      "EXECUTION TIME: \n",
      "0.003\n",
      "\n",
      "\u001b[96m#################################################################\u001b[0m\n",
      "\u001b[96m#######  Environment: LavaFloor-v0 \tValue Iteration  ########\u001b[0m\n",
      "\u001b[96m#################################################################\u001b[0m\n",
      "\n",
      "\u001b[91m> Your policy\n",
      " [['D' 'R' 'D' 'U']\n",
      " ['D' 'L' 'L' 'L']\n",
      " ['R' 'R' 'R' 'L']] is not optimal!\n",
      "\n",
      "Our policy is:\n",
      " [['D' 'L' 'L' 'U']\n",
      " ['D' 'L' 'L' 'L']\n",
      " ['R' 'R' 'R' 'L']]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env_name = \"LavaFloor-v0\"\n",
    "#env_name = \"HugeLavaFloor-v0\"\n",
    "#env_name = \"NiceLavaFloor-v0\"\n",
    "#env_name = \"VeryBadLavaFloor-v0\"\n",
    "\n",
    "env = gym.make(env_name)\n",
    "print(\"\\nENV RENDER:\")\n",
    "env.render()\n",
    "\n",
    "print(\"Transition model for \", env_name, \" : \") #assume transition functions is the same for all states\n",
    "state=0\n",
    "next_state=1\n",
    "for i in range(0,env.action_space.n):\n",
    "    print(\"probability of reaching \", env.state_to_pos(next_state), \"from \", env.state_to_pos(state), \" executing action \", env.actions[i], \" : \", env.T[state, i, next_state])\n",
    "print(\"Reward for non terminal states: \",env.RS[env.pos_to_state(0,0)]) #assume all states have same reward\n",
    "for state in range(0,env.observation_space.n):\n",
    "    if env.grid[state] == \"P\" or env.grid[state] == \"G\":\n",
    "                    print(\"Reward for state :\", env.state_to_pos(state) ,\" (state type: \", env.grid[state],\") : \",env.RS[state])\n",
    "\n",
    "t = timer()\n",
    "policy = value_iteration(env)\n",
    "\n",
    "print(\"\\nEXECUTION TIME: \\n{}\".format(round(timer() - t, 4)))\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "results = CheckResult_L3(env_name, policy_render)\n",
    "results.check_value_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Policy Iteration Algorithm (<span style=\"color:red\">*OPTIONAL*</span>)\n",
    "\n",
    "Your <span style=\"color:red\"> *optional*</span> assignment is to implement the Policy Iteration algorithm on LavaFloor. The solution returned by your algorithm must be a 1-d array of action identifiers where the $i$-th action refers to the $i$-th state. You can perform all the test on a different versions of the environment, but with the same structure: *HugeLavaFloor*, *NiceLavaFloor* and *VeryBadLavaFloor*.\n",
    "\n",
    "<img src=\"images/policy-iteration.png\" width=\"600\">\n",
    "\n",
    "For the *policy evaluation step*, it is necessary to implement this function:\n",
    "\n",
    "<img src=\"images/policy-evaluating.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following function has to be implemented:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(environment, maxiters=150, discount=0.9, maxviter=10):\n",
    "    \"\"\"\n",
    "    Performs the policy iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        maxiters: timeout for the iterations\n",
    "        discount: gamma value, the discount factor for the Bellman equation\n",
    "        \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "    \"\"\"\n",
    "    \n",
    "    policy = [0 for _ in range(environment.observation_space.n)] #initial policy\n",
    "    U = [0 for _ in range(environment.observation_space.n)] #utility array\n",
    "\n",
    "    # Step (1): Policy Evaluation\n",
    "    #\n",
    "    # Code Here!\n",
    "    #\n",
    "    \n",
    "    # Step (2) Policy Improvement\n",
    "    unchanged = True  \n",
    "    #\n",
    "    # Code Here!\n",
    "    #\n",
    "    \n",
    "    return np.asarray(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code executes and Value Iteration and prints the resulting policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"LavaFloor-v0\"\n",
    "#env_name = \"HugeLavaFloor-v0\"\n",
    "#env_name = \"NiceLavaFloor-v0\"\n",
    "#env_name = \"VeryBadLavaFloor-v0\"\n",
    "\n",
    "env = gym.make(env_name)\n",
    "print(\"\\nENV RENDER:\")\n",
    "env.render()\n",
    "\n",
    "print(\"Transition model for \", env_name, \" : \") #assume transition functions is the same for all states\n",
    "state=0\n",
    "next_state=1\n",
    "for i in range(0,env.action_space.n):\n",
    "    print(\"probability of reaching \", env.state_to_pos(next_state), \"from \", env.state_to_pos(state), \" executing action \", env.actions[i], \" : \", env.T[state, i, next_state])\n",
    "print(\"Reward for non terminal states: \",env.RS[env.pos_to_state(0,0)]) #assume all states have same reward\n",
    "for state in range(0,env.observation_space.n):\n",
    "    if env.grid[state] == \"P\" or env.grid[state] == \"G\":\n",
    "                    print(\"Reward for state :\", env.state_to_pos(state) ,\" (state type: \", env.grid[state],\") : \",env.RS[state])\n",
    "\n",
    "t = timer()\n",
    "policy = policy_iteration(env)\n",
    "\n",
    "print(\"\\nEXECUTION TIME: \\n{}\".format(round(timer() - t, 4)))\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "results = CheckResult_L3(env_name, policy_render)\n",
    "results.check_policy_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "The following code performs a comparison between Value Iteration and Policy Iteration, by plotting the accumulated rewards of each episode with iterations in range $[1, 50]$ (might take a long time if not optimizied via numpy). You can perform all the test on a different versions of the environment, but with the same structure: *HugeLavaFloor*.\n",
    "\n",
    "The function **run_episode(envirnonment, policy, max_iteration)** run an episode on the given environment using the input policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"LavaFloor-v0\"\n",
    "#env_name = \"HugeLavaFloor-v0\"\n",
    "\n",
    "print(\"Transition model for \", env_name, \" : \") #assume transition functions is the same for all states\n",
    "state=0\n",
    "next_state=1\n",
    "for i in range(0,env.action_space.n):\n",
    "    print(\"probability of reaching \", env.state_to_pos(next_state), \"from \", env.state_to_pos(state), \" executing action \", env.actions[i], \" : \", env.T[state, i, next_state])\n",
    "print(\"Reward for non terminal states: \",env.RS[env.pos_to_state(0,0)]) #assume all states have same reward\n",
    "for state in range(0,env.observation_space.n):\n",
    "    if env.grid[state] == \"P\" or env.grid[state] == \"G\":\n",
    "                    print(\"Reward for state :\", env.state_to_pos(state) ,\" (state type: \", env.grid[state],\") : \",env.RS[state])\n",
    "\n",
    "\n",
    "\n",
    "maxiters = 49\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "series = []  # Series of learning rates to plot\n",
    "liters = np.arange(maxiters + 1)  # Learning iteration values\n",
    "liters[0] = 1\n",
    "elimit = 100  # Limit of steps per episode\n",
    "rep = 10  # Number of repetitions per iteration value\n",
    "virewards = np.zeros(len(liters))  # Rewards array\n",
    "c = 0\n",
    "\n",
    "t = timer()\n",
    "\n",
    "# Value iteration\n",
    "for i in tqdm(liters, desc=\"Value Iteration\", leave=True):\n",
    "    reprew = 0\n",
    "    policy = value_iteration(env, maxiters=i)  # Compute policy\n",
    "        \n",
    "    # Repeat multiple times and compute mean reward\n",
    "    for _ in range(rep):\n",
    "        reprew += run_episode(env, policy, elimit)  # Execute policy\n",
    "    virewards[c] = reprew / rep\n",
    "    c += 1\n",
    "series.append({\"x\": liters, \"y\": virewards, \"ls\": \"-\", \"label\": \"Value Iteration\"})\n",
    "\n",
    "\n",
    "vmaxiters = 5  # Max number of iterations to perform while evaluating a policy\n",
    "pirewards = np.zeros(len(liters))  # Rewards array\n",
    "c = 0\n",
    "\n",
    "# Policy iteration\n",
    "for i in tqdm(liters, desc=\"Policy Iteration\", leave=True):\n",
    "    reprew = 0\n",
    "    policy = policy_iteration(env, maxiters=i)  # Compute policy\n",
    "    # Repeat multiple times and compute mean reward\n",
    "    for _ in range(rep):\n",
    "        reprew += run_episode(env, policy, elimit)  # Execute policy\n",
    "    pirewards[c] = reprew / rep\n",
    "    c += 1\n",
    "series.append({\"x\": liters, \"y\": pirewards, \"ls\": \"-\", \"label\": \"Policy Iteration\"})\n",
    "\n",
    "print(\"Execution time: {0}s\".format(round(timer() - t, 4)))\n",
    "np.set_printoptions(linewidth=10000)\n",
    "\n",
    "plot(series, \"Learning Rate\", \"Iterations\", \"Reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct results for comparison can be found here below. Notice that since the executions are stochastic the charts could differ: the important thing is the global trend and the final convergence to an optimal solution.\n",
    "\n",
    "**Standard Lava floor results comparison**\n",
    "<img src=\"images/results-standard.png\" width=\"600\">\n",
    "\n",
    "**Huge Lava floor results comparison** \n",
    "<img src=\"images/results-huge.png\" width=\"600\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
